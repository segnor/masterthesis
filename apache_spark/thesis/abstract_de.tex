%%
%% Abstract
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Wird am Ende der Arbeit nochmals überarbeitet/angepasst

\section*{Kurzfassung}

Daten gelten heute als der Rohstoff der Zukunft. Ständig werden extreme Mengen von Daten von den unterschiedlichsten Quellen generiert. Sei es durch Social-Media, durch Suchmaschinen, durch Bewegungs- oder andere Sensordaten, durch Logdateien, etc. Die Menge der generierten Daten steigt von Jahr zu Jahr an - von ca 130 Exabyte\footnote{1 Exabyte = \(10^1^8\) Byte oder eine Million Terabyte.} im Jahre 2005 bis zu 8591 Exabyte für das Jahr 2015 und für 2020 werden Prognosen zufolge sogar über 40000 Exabyte Daten erzeugt \citeint{sta15}. Diese Daten haben wenig miteinander gemeinsam. Es handelt sich um strukturierte oder unstrukturierte Datensätze, um persistente oder um flüchtige Daten aus einem Strom. Für all diese Daten sollen aber vergleichbare Werkzeuge zur Analyse und Verarbeitung verfügbar sein. Ein Ansatz für dieses Problem liefert Apache Spark mit dem dazugehörigen Berkeley Data Analytics Stack. Dies sind Werkzeuge für Analyse massiver Datenmengen, Verarbeitung von flüchtigen Streamingdaten, Batchverarbeitung, SQL-Abfragen und verschiedenen Aufgaben aus dem Bereich des Machine Learning. In dieser Thesis werden die Bibliotheken des Berkeley Data Analytics Stack vorgestellt, gegenüber Alternativimplementierungen verglichen und Messungen unterzogen. Für diesen Zweck wurden Metriken definiert, sowie Testumgebungen aufgesetzt und Prototypen für die jeweiligen Bibliotheken implementiert.   


%% eof
